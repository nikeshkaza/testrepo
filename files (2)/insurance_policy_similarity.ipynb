{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insurance Policy Similarity Matching System\n",
    "\n",
    "## Business Objective\n",
    "Build a model to find top 3 similar historical/renewal policies for new business submissions to assist underwriters in risk analysis.\n",
    "\n",
    "## Approach\n",
    "1. **Feature Engineering**: Transform numerical and categorical features\n",
    "2. **Dimensionality Reduction**: Use PCA/UMAP for better clustering\n",
    "3. **Clustering**: Apply multiple algorithms (K-Means, DBSCAN, Hierarchical)\n",
    "4. **Similarity Search**: Build nearest neighbor search for policy matching\n",
    "5. **Evaluation**: Use silhouette score, Davies-Bouldin index, and business metrics\n",
    "6. **Explainability**: SHAP values to understand feature importance in similarity\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install scikit-learn pandas numpy matplotlib seaborn umap-learn shap plotly category_encoders --quiet\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from category_encoders import TargetEncoder\n",
    "\n",
    "# Clustering\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# Similarity Search\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import (\n",
    "    silhouette_score, \n",
    "    davies_bouldin_score, \n",
    "    calinski_harabasz_score\n",
    ")\n",
    "\n",
    "# Dimensionality Reduction\n",
    "import umap\n",
    "\n",
    "# Explainability\n",
    "import shap\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading & Initial Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features based on your data\n",
    "numerical_features = [\n",
    "    'DUNS_NUMBER_1', 'policy_tiv', 'Revenue', 'highest_location_tiv', \n",
    "    'POSTAL_CD', 'LAT_NEW', 'LATIT', 'LONGIT', 'LONG_NEW', 'SIC_1', \n",
    "    'EMP_TOT', 'SLES_VOL', 'YR_STRT', 'STAT_IND', 'SUBS_IND', 'outliers'\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    'System Reference Number', 'Effective Date', 'Expiration Date', \n",
    "    '2012 NAIC Code', '2012 NAIC Description', 'Programme Type', \n",
    "    'Portfolio Class Code', 'Portfolio Segmentation', 'Producer Code', \n",
    "    'Producer', 'LOCATION', 'COMPANY_NAME', 'ADDR', 'Product', \n",
    "    'Sub Product', '[Process Type]', 'Position Type', 'Placement Type', \n",
    "    'Practice/Non-Practice', 'Revised Practice', 'Policy Holder Name', \n",
    "    'Policy Industry Code', 'Policy Industry Description', \n",
    "    '2012 NAIC 2 Digit Code', '2012 NAIC 2 Digit Description', \n",
    "    '2012 NAIC 3 Digit Code', '2012 NAIC 3 Digit Description', \n",
    "    '2012 NAIC 4 Digit Code', '2012 NAIC 4 Digit Description', \n",
    "    'Policy Number', 'Short Tail / Long Tail'\n",
    "]\n",
    "\n",
    "# Load your data\n",
    "# df = pd.read_csv('your_insurance_data.csv')\n",
    "\n",
    "# For demonstration, create synthetic data\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "# Create synthetic dataset\n",
    "data = {}\n",
    "\n",
    "# Numerical features\n",
    "data['DUNS_NUMBER_1'] = np.random.randint(100000000, 999999999, n_samples)\n",
    "data['policy_tiv'] = np.random.lognormal(15, 2, n_samples)\n",
    "data['Revenue'] = np.random.lognormal(16, 1.5, n_samples)\n",
    "data['highest_location_tiv'] = data['policy_tiv'] * np.random.uniform(0.1, 0.8, n_samples)\n",
    "data['POSTAL_CD'] = np.random.randint(10000, 99999, n_samples)\n",
    "data['LAT_NEW'] = np.random.uniform(25, 48, n_samples)\n",
    "data['LATIT'] = data['LAT_NEW'] + np.random.normal(0, 0.1, n_samples)\n",
    "data['LONG_NEW'] = np.random.uniform(-120, -70, n_samples)\n",
    "data['LONGIT'] = data['LONG_NEW'] + np.random.normal(0, 0.1, n_samples)\n",
    "data['SIC_1'] = np.random.choice([1731, 3571, 5411, 7372, 8062], n_samples)\n",
    "data['EMP_TOT'] = np.random.lognormal(5, 2, n_samples).astype(int)\n",
    "data['SLES_VOL'] = data['Revenue'] * np.random.uniform(0.8, 1.2, n_samples)\n",
    "data['YR_STRT'] = np.random.randint(1950, 2020, n_samples)\n",
    "data['STAT_IND'] = np.random.choice([0, 1], n_samples)\n",
    "data['SUBS_IND'] = np.random.choice([0, 1], n_samples)\n",
    "data['outliers'] = np.random.choice([0, 1], n_samples, p=[0.95, 0.05])\n",
    "\n",
    "# Key categorical features (simplified)\n",
    "data['Product'] = np.random.choice(['Property', 'Casualty', 'Professional Liability'], n_samples)\n",
    "data['Sub Product'] = np.random.choice(['General Liability', 'E&O', 'D&O', 'Property Damage'], n_samples)\n",
    "data['Portfolio Segmentation'] = np.random.choice(['Small', 'Medium', 'Large', 'Enterprise'], n_samples)\n",
    "data['Programme Type'] = np.random.choice(['Standard', 'Package', 'Umbrella'], n_samples)\n",
    "data['Short Tail / Long Tail'] = np.random.choice(['Short Tail', 'Long Tail'], n_samples)\n",
    "data['Policy Industry Description'] = np.random.choice(\n",
    "    ['Manufacturing', 'Technology', 'Healthcare', 'Retail', 'Construction'], n_samples\n",
    ")\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(f\"Dataset Shape: {df.shape}\")\n",
    "print(f\"\\nNumerical Features: {len(numerical_features)}\")\n",
    "print(f\"Categorical Features: {len(categorical_features)} (using subset for demo)\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Quality Check\n",
    "print(\"Missing Values:\")\n",
    "print(df.isnull().sum()[df.isnull().sum() > 0])\n",
    "\n",
    "print(\"\\nData Types:\")\n",
    "print(df.dtypes.value_counts())\n",
    "\n",
    "print(\"\\nBasic Statistics:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InsurancePolicyPreprocessor:\n",
    "    \"\"\"\n",
    "    Comprehensive preprocessor for insurance policy data\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.numerical_scaler = RobustScaler()  # Robust to outliers\n",
    "        self.categorical_encoders = {}\n",
    "        self.feature_names = []\n",
    "        \n",
    "    def engineer_features(self, df):\n",
    "        \"\"\"Create derived features\"\"\"\n",
    "        df_eng = df.copy()\n",
    "        \n",
    "        # Financial ratios\n",
    "        df_eng['tiv_per_location'] = df_eng['policy_tiv'] / (df_eng['highest_location_tiv'] + 1)\n",
    "        df_eng['revenue_per_employee'] = df_eng['Revenue'] / (df_eng['EMP_TOT'] + 1)\n",
    "        df_eng['sales_to_revenue_ratio'] = df_eng['SLES_VOL'] / (df_eng['Revenue'] + 1)\n",
    "        \n",
    "        # Company age\n",
    "        current_year = 2024\n",
    "        df_eng['company_age'] = current_year - df_eng['YR_STRT']\n",
    "        \n",
    "        # Coordinate features\n",
    "        df_eng['lat_consistency'] = np.abs(df_eng['LAT_NEW'] - df_eng['LATIT'])\n",
    "        df_eng['long_consistency'] = np.abs(df_eng['LONG_NEW'] - df_eng['LONGIT'])\n",
    "        \n",
    "        # Log transforms for skewed features\n",
    "        df_eng['log_policy_tiv'] = np.log1p(df_eng['policy_tiv'])\n",
    "        df_eng['log_revenue'] = np.log1p(df_eng['Revenue'])\n",
    "        df_eng['log_employees'] = np.log1p(df_eng['EMP_TOT'])\n",
    "        \n",
    "        return df_eng\n",
    "    \n",
    "    def encode_categorical(self, df, categorical_cols):\n",
    "        \"\"\"Encode categorical variables using multiple strategies\"\"\"\n",
    "        df_encoded = df.copy()\n",
    "        encoded_features = []\n",
    "        \n",
    "        for col in categorical_cols:\n",
    "            if col in df.columns:\n",
    "                # Frequency encoding\n",
    "                freq_encoding = df[col].value_counts(normalize=True).to_dict()\n",
    "                df_encoded[f'{col}_freq'] = df[col].map(freq_encoding)\n",
    "                encoded_features.append(f'{col}_freq')\n",
    "                \n",
    "                # Label encoding for low cardinality\n",
    "                if df[col].nunique() < 50:\n",
    "                    le = LabelEncoder()\n",
    "                    df_encoded[f'{col}_label'] = le.fit_transform(df[col].astype(str))\n",
    "                    self.categorical_encoders[col] = le\n",
    "                    encoded_features.append(f'{col}_label')\n",
    "        \n",
    "        return df_encoded, encoded_features\n",
    "    \n",
    "    def fit_transform(self, df, numerical_cols, categorical_cols):\n",
    "        \"\"\"Complete preprocessing pipeline\"\"\"\n",
    "        # Engineer features\n",
    "        df_processed = self.engineer_features(df)\n",
    "        \n",
    "        # Encode categorical\n",
    "        df_processed, cat_encoded_features = self.encode_categorical(\n",
    "            df_processed, categorical_cols\n",
    "        )\n",
    "        \n",
    "        # Prepare numerical features\n",
    "        num_cols_available = [col for col in numerical_cols if col in df_processed.columns]\n",
    "        engineered_num_cols = [\n",
    "            'tiv_per_location', 'revenue_per_employee', 'sales_to_revenue_ratio',\n",
    "            'company_age', 'lat_consistency', 'long_consistency',\n",
    "            'log_policy_tiv', 'log_revenue', 'log_employees'\n",
    "        ]\n",
    "        \n",
    "        all_num_cols = num_cols_available + engineered_num_cols\n",
    "        \n",
    "        # Handle missing values\n",
    "        df_processed[all_num_cols] = df_processed[all_num_cols].fillna(\n",
    "            df_processed[all_num_cols].median()\n",
    "        )\n",
    "        df_processed[cat_encoded_features] = df_processed[cat_encoded_features].fillna(0)\n",
    "        \n",
    "        # Scale numerical features\n",
    "        X_numerical = self.numerical_scaler.fit_transform(df_processed[all_num_cols])\n",
    "        X_categorical = df_processed[cat_encoded_features].values\n",
    "        \n",
    "        # Combine features\n",
    "        X_combined = np.hstack([X_numerical, X_categorical])\n",
    "        \n",
    "        # Store feature names\n",
    "        self.feature_names = all_num_cols + cat_encoded_features\n",
    "        \n",
    "        return X_combined, df_processed\n",
    "\n",
    "# Apply preprocessing\n",
    "preprocessor = InsurancePolicyPreprocessor()\n",
    "\n",
    "# Use available categorical features from our demo data\n",
    "available_categorical = [\n",
    "    'Product', 'Sub Product', 'Portfolio Segmentation', \n",
    "    'Programme Type', 'Short Tail / Long Tail', 'Policy Industry Description'\n",
    "]\n",
    "\n",
    "X, df_processed = preprocessor.fit_transform(\n",
    "    df, \n",
    "    numerical_features, \n",
    "    available_categorical\n",
    ")\n",
    "\n",
    "print(f\"Processed Feature Matrix Shape: {X.shape}\")\n",
    "print(f\"Total Features: {len(preprocessor.feature_names)}\")\n",
    "print(f\"\\nFeature Names: {preprocessor.feature_names[:10]}...\")  # Show first 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA for variance analysis\n",
    "pca_full = PCA()\n",
    "X_pca_full = pca_full.fit_transform(X)\n",
    "\n",
    "# Plot explained variance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Cumulative variance\n",
    "cumsum_var = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "axes[0].plot(range(1, len(cumsum_var) + 1), cumsum_var, 'b-', linewidth=2)\n",
    "axes[0].axhline(y=0.95, color='r', linestyle='--', label='95% Variance')\n",
    "axes[0].axhline(y=0.90, color='g', linestyle='--', label='90% Variance')\n",
    "axes[0].set_xlabel('Number of Components')\n",
    "axes[0].set_ylabel('Cumulative Explained Variance')\n",
    "axes[0].set_title('PCA Cumulative Explained Variance')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Individual variance\n",
    "axes[1].bar(range(1, min(21, len(pca_full.explained_variance_ratio_) + 1)), \n",
    "            pca_full.explained_variance_ratio_[:20])\n",
    "axes[1].set_xlabel('Principal Component')\n",
    "axes[1].set_ylabel('Explained Variance Ratio')\n",
    "axes[1].set_title('Variance Explained by Each Component (First 20)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Determine optimal number of components\n",
    "n_components_95 = np.argmax(cumsum_var >= 0.95) + 1\n",
    "n_components_90 = np.argmax(cumsum_var >= 0.90) + 1\n",
    "\n",
    "print(f\"Components needed for 90% variance: {n_components_90}\")\n",
    "print(f\"Components needed for 95% variance: {n_components_95}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA and UMAP for clustering\n",
    "n_components_pca = min(50, X.shape[1])  # Use 50 components or less\n",
    "\n",
    "pca = PCA(n_components=n_components_pca, random_state=42)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "print(f\"PCA: Reduced to {n_components_pca} components\")\n",
    "print(f\"Variance retained: {pca.explained_variance_ratio_.sum():.2%}\")\n",
    "\n",
    "# UMAP for visualization and clustering\n",
    "umap_reducer = umap.UMAP(\n",
    "    n_components=2, \n",
    "    n_neighbors=15, \n",
    "    min_dist=0.1, \n",
    "    metric='euclidean',\n",
    "    random_state=42\n",
    ")\n",
    "X_umap_2d = umap_reducer.fit_transform(X_pca)\n",
    "\n",
    "# UMAP for clustering (higher dimensions)\n",
    "umap_reducer_cluster = umap.UMAP(\n",
    "    n_components=10, \n",
    "    n_neighbors=15, \n",
    "    min_dist=0.1, \n",
    "    metric='euclidean',\n",
    "    random_state=42\n",
    ")\n",
    "X_umap = umap_reducer_cluster.fit_transform(X_pca)\n",
    "\n",
    "print(f\"\\nUMAP: Reduced to 10 components for clustering\")\n",
    "print(f\"UMAP: Created 2D projection for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Clustering Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine optimal number of clusters using Elbow Method and Silhouette Score\n",
    "K_range = range(3, 15)\n",
    "inertias = []\n",
    "silhouette_scores = []\n",
    "davies_bouldin_scores = []\n",
    "\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    labels = kmeans.fit_predict(X_umap)\n",
    "    \n",
    "    inertias.append(kmeans.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(X_umap, labels))\n",
    "    davies_bouldin_scores.append(davies_bouldin_score(X_umap, labels))\n",
    "\n",
    "# Plot cluster evaluation metrics\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Elbow plot\n",
    "axes[0].plot(K_range, inertias, 'bo-', linewidth=2, markersize=8)\n",
    "axes[0].set_xlabel('Number of Clusters (K)', fontsize=12)\n",
    "axes[0].set_ylabel('Inertia', fontsize=12)\n",
    "axes[0].set_title('Elbow Method', fontsize=14)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Silhouette score (higher is better)\n",
    "axes[1].plot(K_range, silhouette_scores, 'go-', linewidth=2, markersize=8)\n",
    "axes[1].set_xlabel('Number of Clusters (K)', fontsize=12)\n",
    "axes[1].set_ylabel('Silhouette Score', fontsize=12)\n",
    "axes[1].set_title('Silhouette Score (Higher is Better)', fontsize=14)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Davies-Bouldin score (lower is better)\n",
    "axes[2].plot(K_range, davies_bouldin_scores, 'ro-', linewidth=2, markersize=8)\n",
    "axes[2].set_xlabel('Number of Clusters (K)', fontsize=12)\n",
    "axes[2].set_ylabel('Davies-Bouldin Score', fontsize=12)\n",
    "axes[2].set_title('Davies-Bouldin Score (Lower is Better)', fontsize=14)\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find optimal K\n",
    "optimal_k = K_range[np.argmax(silhouette_scores)]\n",
    "print(f\"\\nOptimal number of clusters: {optimal_k}\")\n",
    "print(f\"Best Silhouette Score: {max(silhouette_scores):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply multiple clustering algorithms\n",
    "clustering_results = {}\n",
    "\n",
    "# 1. K-Means\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=20)\n",
    "clustering_results['kmeans'] = kmeans.fit_predict(X_umap)\n",
    "\n",
    "# 2. Hierarchical Clustering\n",
    "hierarchical = AgglomerativeClustering(n_clusters=optimal_k)\n",
    "clustering_results['hierarchical'] = hierarchical.fit_predict(X_umap)\n",
    "\n",
    "# 3. Gaussian Mixture Model\n",
    "gmm = GaussianMixture(n_components=optimal_k, random_state=42)\n",
    "clustering_results['gmm'] = gmm.fit_predict(X_umap)\n",
    "\n",
    "# 4. DBSCAN (density-based)\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "clustering_results['dbscan'] = dbscan.fit_predict(X_umap)\n",
    "\n",
    "# Compare clustering algorithms\n",
    "print(\"Clustering Algorithm Comparison:\\n\")\n",
    "print(f\"{'Algorithm':<15} {'Silhouette':<12} {'Davies-Bouldin':<18} {'Calinski-Harabasz':<20} {'N Clusters'}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for name, labels in clustering_results.items():\n",
    "    n_clusters = len(np.unique(labels[labels >= 0]))  # Exclude noise for DBSCAN\n",
    "    \n",
    "    if n_clusters > 1:  # Need at least 2 clusters for metrics\n",
    "        sil_score = silhouette_score(X_umap, labels)\n",
    "        db_score = davies_bouldin_score(X_umap, labels)\n",
    "        ch_score = calinski_harabasz_score(X_umap, labels)\n",
    "        \n",
    "        print(f\"{name:<15} {sil_score:<12.3f} {db_score:<18.3f} {ch_score:<20.1f} {n_clusters}\")\n",
    "    else:\n",
    "        print(f\"{name:<15} {'N/A':<12} {'N/A':<18} {'N/A':<20} {n_clusters}\")\n",
    "\n",
    "# Use K-Means as primary clustering (generally best for this use case)\n",
    "primary_labels = clustering_results['kmeans']\n",
    "df_processed['cluster'] = primary_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize clusters in 2D UMAP space\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, (name, labels) in enumerate(clustering_results.items()):\n",
    "    scatter = axes[idx].scatter(\n",
    "        X_umap_2d[:, 0], \n",
    "        X_umap_2d[:, 1], \n",
    "        c=labels, \n",
    "        cmap='tab10', \n",
    "        alpha=0.6,\n",
    "        s=30\n",
    "    )\n",
    "    axes[idx].set_title(f'{name.upper()} Clustering', fontsize=14, fontweight='bold')\n",
    "    axes[idx].set_xlabel('UMAP Component 1', fontsize=11)\n",
    "    axes[idx].set_ylabel('UMAP Component 2', fontsize=11)\n",
    "    plt.colorbar(scatter, ax=axes[idx], label='Cluster')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cluster Profiling & Business Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze cluster characteristics\n",
    "def profile_clusters(df, cluster_col='cluster'):\n",
    "    \"\"\"\n",
    "    Create comprehensive cluster profiles\n",
    "    \"\"\"\n",
    "    profiles = []\n",
    "    \n",
    "    for cluster_id in sorted(df[cluster_col].unique()):\n",
    "        cluster_data = df[df[cluster_col] == cluster_id]\n",
    "        \n",
    "        profile = {\n",
    "            'Cluster': cluster_id,\n",
    "            'Size': len(cluster_data),\n",
    "            'Size %': f\"{len(cluster_data) / len(df) * 100:.1f}%\",\n",
    "            'Avg TIV': f\"${cluster_data['policy_tiv'].mean():,.0f}\",\n",
    "            'Avg Revenue': f\"${cluster_data['Revenue'].mean():,.0f}\",\n",
    "            'Avg Employees': f\"{cluster_data['EMP_TOT'].mean():.0f}\",\n",
    "            'Top Product': cluster_data['Product'].mode()[0] if 'Product' in cluster_data.columns else 'N/A',\n",
    "            'Top Industry': cluster_data['Policy Industry Description'].mode()[0] if 'Policy Industry Description' in cluster_data.columns else 'N/A',\n",
    "        }\n",
    "        profiles.append(profile)\n",
    "    \n",
    "    return pd.DataFrame(profiles)\n",
    "\n",
    "cluster_profiles = profile_clusters(df_processed)\n",
    "print(\"\\nCluster Profiles:\")\n",
    "print(cluster_profiles.to_string(index=False))\n",
    "\n",
    "# Visualize cluster distributions\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# Key numerical features by cluster\n",
    "features_to_plot = ['policy_tiv', 'Revenue', 'EMP_TOT', 'company_age', \n",
    "                     'revenue_per_employee', 'sales_to_revenue_ratio']\n",
    "\n",
    "for idx, feature in enumerate(features_to_plot):\n",
    "    ax = axes[idx // 3, idx % 3]\n",
    "    \n",
    "    df_processed.boxplot(\n",
    "        column=feature, \n",
    "        by='cluster', \n",
    "        ax=ax,\n",
    "        patch_artist=True\n",
    "    )\n",
    "    ax.set_title(f'{feature} by Cluster')\n",
    "    ax.set_xlabel('Cluster')\n",
    "    ax.set_ylabel(feature)\n",
    "    plt.sca(ax)\n",
    "    plt.xticks(rotation=0)\n",
    "\n",
    "plt.suptitle('Feature Distributions by Cluster', fontsize=16, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Similarity Search System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicySimilarityMatcher:\n",
    "    \"\"\"\n",
    "    Find similar insurance policies using nearest neighbor search\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, X_features, df_original, metric='euclidean', n_neighbors=3):\n",
    "        self.X = X_features\n",
    "        self.df = df_original\n",
    "        self.metric = metric\n",
    "        self.n_neighbors = n_neighbors\n",
    "        \n",
    "        # Build nearest neighbor model\n",
    "        self.nn_model = NearestNeighbors(\n",
    "            n_neighbors=n_neighbors + 1,  # +1 to exclude the query itself\n",
    "            metric=metric,\n",
    "            algorithm='auto'\n",
    "        )\n",
    "        self.nn_model.fit(X_features)\n",
    "        \n",
    "    def find_similar_policies(self, new_policy_features, return_distances=True):\n",
    "        \"\"\"\n",
    "        Find top N similar policies for a new policy\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        new_policy_features : array-like\n",
    "            Preprocessed features of the new policy\n",
    "        return_distances : bool\n",
    "            Whether to return similarity distances\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        DataFrame with similar policies and their details\n",
    "        \"\"\"\n",
    "        # Reshape if needed\n",
    "        if new_policy_features.ndim == 1:\n",
    "            new_policy_features = new_policy_features.reshape(1, -1)\n",
    "        \n",
    "        # Find nearest neighbors\n",
    "        distances, indices = self.nn_model.kneighbors(new_policy_features)\n",
    "        \n",
    "        # Get similar policies\n",
    "        similar_policies = self.df.iloc[indices[0]].copy()\n",
    "        \n",
    "        if return_distances:\n",
    "            similar_policies['similarity_distance'] = distances[0]\n",
    "            similar_policies['similarity_score'] = 1 / (1 + distances[0])  # Convert to similarity\n",
    "        \n",
    "        return similar_policies\n",
    "    \n",
    "    def batch_find_similar(self, test_indices, top_n=3):\n",
    "        \"\"\"\n",
    "        Find similar policies for multiple test cases\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for idx in test_indices:\n",
    "            query_features = self.X[idx].reshape(1, -1)\n",
    "            similar = self.find_similar_policies(query_features)\n",
    "            \n",
    "            # Exclude the query itself\n",
    "            similar = similar.iloc[1:top_n+1]\n",
    "            \n",
    "            results.append({\n",
    "                'query_idx': idx,\n",
    "                'similar_policies': similar\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Initialize similarity matcher\n",
    "matcher = PolicySimilarityMatcher(\n",
    "    X_umap,  # Use UMAP-reduced features for matching\n",
    "    df_processed,\n",
    "    metric='euclidean',\n",
    "    n_neighbors=3\n",
    ")\n",
    "\n",
    "print(\"Policy Similarity Matcher initialized successfully!\")\n",
    "print(f\"Search space: {X_umap.shape[0]} policies\")\n",
    "print(f\"Feature dimensions: {X_umap.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration: Find similar policies for sample cases\n",
    "sample_indices = np.random.choice(len(df_processed), 3, replace=False)\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"POLICY SIMILARITY MATCHING DEMONSTRATION\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "for i, idx in enumerate(sample_indices, 1):\n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(f\"QUERY POLICY #{i} (Index: {idx})\")\n",
    "    print(f\"{'='*100}\")\n",
    "    \n",
    "    query_policy = df_processed.iloc[idx]\n",
    "    \n",
    "    print(f\"\\nQuery Policy Details:\")\n",
    "    print(f\"  TIV: ${query_policy['policy_tiv']:,.0f}\")\n",
    "    print(f\"  Revenue: ${query_policy['Revenue']:,.0f}\")\n",
    "    print(f\"  Employees: {query_policy['EMP_TOT']:.0f}\")\n",
    "    print(f\"  Product: {query_policy.get('Product', 'N/A')}\")\n",
    "    print(f\"  Industry: {query_policy.get('Policy Industry Description', 'N/A')}\")\n",
    "    print(f\"  Cluster: {query_policy['cluster']}\")\n",
    "    \n",
    "    # Find similar policies\n",
    "    query_features = X_umap[idx].reshape(1, -1)\n",
    "    similar = matcher.find_similar_policies(query_features)\n",
    "    \n",
    "    # Exclude the query itself (first result)\n",
    "    similar = similar.iloc[1:4]\n",
    "    \n",
    "    print(f\"\\n{'-'*100}\")\n",
    "    print(f\"TOP 3 SIMILAR POLICIES FOR UNDERWRITER REVIEW:\")\n",
    "    print(f\"{'-'*100}\")\n",
    "    \n",
    "    for j, (_, policy) in enumerate(similar.iterrows(), 1):\n",
    "        print(f\"\\n  Rank {j} - Similarity Score: {policy['similarity_score']:.3f}\")\n",
    "        print(f\"    TIV: ${policy['policy_tiv']:,.0f} (vs ${query_policy['policy_tiv']:,.0f})\")\n",
    "        print(f\"    Revenue: ${policy['Revenue']:,.0f} (vs ${query_policy['Revenue']:,.0f})\")\n",
    "        print(f\"    Employees: {policy['EMP_TOT']:.0f} (vs {query_policy['EMP_TOT']:.0f})\")\n",
    "        print(f\"    Product: {policy.get('Product', 'N/A')}\")\n",
    "        print(f\"    Industry: {policy.get('Policy Industry Description', 'N/A')}\")\n",
    "        print(f\"    Cluster: {policy['cluster']}\")\n",
    "        print(f\"    Distance: {policy['similarity_distance']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Validation & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate similarity matching quality\n",
    "def evaluate_similarity_matching(matcher, X, df, n_samples=100):\n",
    "    \"\"\"\n",
    "    Evaluate the quality of similarity matching\n",
    "    \"\"\"\n",
    "    test_indices = np.random.choice(len(df), n_samples, replace=False)\n",
    "    \n",
    "    cluster_consistency_scores = []\n",
    "    product_consistency_scores = []\n",
    "    industry_consistency_scores = []\n",
    "    avg_distances = []\n",
    "    \n",
    "    for idx in test_indices:\n",
    "        query_features = X[idx].reshape(1, -1)\n",
    "        similar = matcher.find_similar_policies(query_features)\n",
    "        \n",
    "        # Exclude query itself\n",
    "        similar = similar.iloc[1:4]\n",
    "        \n",
    "        query_policy = df.iloc[idx]\n",
    "        \n",
    "        # Cluster consistency\n",
    "        same_cluster = (similar['cluster'] == query_policy['cluster']).sum()\n",
    "        cluster_consistency_scores.append(same_cluster / 3)\n",
    "        \n",
    "        # Product consistency\n",
    "        if 'Product' in df.columns:\n",
    "            same_product = (similar['Product'] == query_policy['Product']).sum()\n",
    "            product_consistency_scores.append(same_product / 3)\n",
    "        \n",
    "        # Industry consistency\n",
    "        if 'Policy Industry Description' in df.columns:\n",
    "            same_industry = (\n",
    "                similar['Policy Industry Description'] == query_policy['Policy Industry Description']\n",
    "            ).sum()\n",
    "            industry_consistency_scores.append(same_industry / 3)\n",
    "        \n",
    "        # Average distance\n",
    "        avg_distances.append(similar['similarity_distance'].mean())\n",
    "    \n",
    "    results = {\n",
    "        'Cluster Consistency': np.mean(cluster_consistency_scores),\n",
    "        'Product Consistency': np.mean(product_consistency_scores) if product_consistency_scores else 0,\n",
    "        'Industry Consistency': np.mean(industry_consistency_scores) if industry_consistency_scores else 0,\n",
    "        'Avg Distance': np.mean(avg_distances),\n",
    "        'Std Distance': np.std(avg_distances)\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run evaluation\n",
    "eval_results = evaluate_similarity_matching(matcher, X_umap, df_processed, n_samples=200)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SIMILARITY MATCHING EVALUATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nCluster Consistency:    {eval_results['Cluster Consistency']:.2%}\")\n",
    "print(f\"  â†’ {eval_results['Cluster Consistency']:.1%} of similar policies are from the same cluster\")\n",
    "\n",
    "print(f\"\\nProduct Consistency:    {eval_results['Product Consistency']:.2%}\")\n",
    "print(f\"  â†’ {eval_results['Product Consistency']:.1%} of similar policies have the same product type\")\n",
    "\n",
    "print(f\"\\nIndustry Consistency:   {eval_results['Industry Consistency']:.2%}\")\n",
    "print(f\"  â†’ {eval_results['Industry Consistency']:.1%} of similar policies are from the same industry\")\n",
    "\n",
    "print(f\"\\nAverage Distance:       {eval_results['Avg Distance']:.4f} Â± {eval_results['Std Distance']:.4f}\")\n",
    "print(f\"  â†’ Lower distance indicates higher similarity\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Business-relevant evaluation: TIV similarity\n",
    "def evaluate_business_metrics(matcher, X, df, n_samples=100):\n",
    "    \"\"\"\n",
    "    Evaluate similarity from business perspective\n",
    "    \"\"\"\n",
    "    test_indices = np.random.choice(len(df), n_samples, replace=False)\n",
    "    \n",
    "    tiv_differences = []\n",
    "    revenue_differences = []\n",
    "    employee_differences = []\n",
    "    \n",
    "    for idx in test_indices:\n",
    "        query_features = X[idx].reshape(1, -1)\n",
    "        similar = matcher.find_similar_policies(query_features)\n",
    "        similar = similar.iloc[1:4]  # Exclude query\n",
    "        \n",
    "        query_policy = df.iloc[idx]\n",
    "        \n",
    "        # Calculate relative differences\n",
    "        tiv_diff = np.abs(similar['policy_tiv'] - query_policy['policy_tiv']) / query_policy['policy_tiv']\n",
    "        tiv_differences.extend(tiv_diff.values)\n",
    "        \n",
    "        rev_diff = np.abs(similar['Revenue'] - query_policy['Revenue']) / query_policy['Revenue']\n",
    "        revenue_differences.extend(rev_diff.values)\n",
    "        \n",
    "        emp_diff = np.abs(similar['EMP_TOT'] - query_policy['EMP_TOT']) / (query_policy['EMP_TOT'] + 1)\n",
    "        employee_differences.extend(emp_diff.values)\n",
    "    \n",
    "    # Visualize distributions\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    axes[0].hist(tiv_differences, bins=30, edgecolor='black', alpha=0.7)\n",
    "    axes[0].axvline(np.median(tiv_differences), color='red', linestyle='--', \n",
    "                    linewidth=2, label=f'Median: {np.median(tiv_differences):.2%}')\n",
    "    axes[0].set_xlabel('Relative TIV Difference', fontsize=12)\n",
    "    axes[0].set_ylabel('Frequency', fontsize=12)\n",
    "    axes[0].set_title('TIV Similarity Distribution', fontsize=14)\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1].hist(revenue_differences, bins=30, edgecolor='black', alpha=0.7, color='green')\n",
    "    axes[1].axvline(np.median(revenue_differences), color='red', linestyle='--', \n",
    "                    linewidth=2, label=f'Median: {np.median(revenue_differences):.2%}')\n",
    "    axes[1].set_xlabel('Relative Revenue Difference', fontsize=12)\n",
    "    axes[1].set_ylabel('Frequency', fontsize=12)\n",
    "    axes[1].set_title('Revenue Similarity Distribution', fontsize=14)\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[2].hist(employee_differences, bins=30, edgecolor='black', alpha=0.7, color='orange')\n",
    "    axes[2].axvline(np.median(employee_differences), color='red', linestyle='--', \n",
    "                    linewidth=2, label=f'Median: {np.median(employee_differences):.2%}')\n",
    "    axes[2].set_xlabel('Relative Employee Count Difference', fontsize=12)\n",
    "    axes[2].set_ylabel('Frequency', fontsize=12)\n",
    "    axes[2].set_title('Employee Count Similarity Distribution', fontsize=14)\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nBusiness Metrics Summary:\")\n",
    "    print(f\"TIV - Median Difference: {np.median(tiv_differences):.2%}\")\n",
    "    print(f\"Revenue - Median Difference: {np.median(revenue_differences):.2%}\")\n",
    "    print(f\"Employees - Median Difference: {np.median(employee_differences):.2%}\")\n",
    "\n",
    "evaluate_business_metrics(matcher, X_umap, df_processed, n_samples=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Explainability with SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a model to predict cluster assignments for SHAP analysis\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Train a Random Forest to predict clusters\n",
    "rf_explainer = RandomForestClassifier(\n",
    "    n_estimators=100, \n",
    "    max_depth=10, \n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_explainer.fit(X, primary_labels)\n",
    "\n",
    "print(f\"Random Forest Accuracy: {rf_explainer.score(X, primary_labels):.2%}\")\n",
    "\n",
    "# Feature importance from Random Forest\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': preprocessor.feature_names,\n",
    "    'importance': rf_explainer.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Plot top 20 features\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_20 = feature_importance.head(20)\n",
    "plt.barh(range(len(top_20)), top_20['importance'])\n",
    "plt.yticks(range(len(top_20)), top_20['feature'])\n",
    "plt.xlabel('Importance', fontsize=12)\n",
    "plt.ylabel('Feature', fontsize=12)\n",
    "plt.title('Top 20 Most Important Features for Clustering', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "print(feature_importance.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Analysis\n",
    "print(\"Computing SHAP values... (this may take a few minutes)\")\n",
    "\n",
    "# Use a sample for SHAP to speed up computation\n",
    "sample_size = min(100, len(X))\n",
    "sample_indices = np.random.choice(len(X), sample_size, replace=False)\n",
    "X_sample = X[sample_indices]\n",
    "\n",
    "# Create SHAP explainer\n",
    "explainer = shap.TreeExplainer(rf_explainer)\n",
    "shap_values = explainer.shap_values(X_sample)\n",
    "\n",
    "print(\"SHAP computation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Summary Plot - shows feature importance and impact\n",
    "plt.figure(figsize=(12, 8))\n",
    "shap.summary_plot(\n",
    "    shap_values, \n",
    "    X_sample, \n",
    "    feature_names=preprocessor.feature_names,\n",
    "    plot_type=\"bar\",\n",
    "    max_display=20,\n",
    "    show=False\n",
    ")\n",
    "plt.title('SHAP Feature Importance (Mean Absolute SHAP Value)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Beeswarm Plot - shows impact and distribution\n",
    "# Select one cluster for detailed analysis\n",
    "target_cluster = 0\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "shap.summary_plot(\n",
    "    shap_values[target_cluster], \n",
    "    X_sample, \n",
    "    feature_names=preprocessor.feature_names,\n",
    "    max_display=20,\n",
    "    show=False\n",
    ")\n",
    "plt.title(f'SHAP Values for Cluster {target_cluster} (Feature Impact)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual prediction explanation\n",
    "# Explain a specific policy\n",
    "policy_idx = sample_indices[0]\n",
    "single_policy = X[policy_idx:policy_idx+1]\n",
    "\n",
    "# Compute SHAP values for this policy\n",
    "shap_values_single = explainer.shap_values(single_policy)\n",
    "\n",
    "# Get prediction\n",
    "predicted_cluster = rf_explainer.predict(single_policy)[0]\n",
    "\n",
    "print(f\"\\nExplaining Policy at Index {policy_idx}\")\n",
    "print(f\"Predicted Cluster: {predicted_cluster}\")\n",
    "print(f\"\\nPolicy Details:\")\n",
    "policy_details = df_processed.iloc[policy_idx]\n",
    "print(f\"  TIV: ${policy_details['policy_tiv']:,.0f}\")\n",
    "print(f\"  Revenue: ${policy_details['Revenue']:,.0f}\")\n",
    "print(f\"  Employees: {policy_details['EMP_TOT']:.0f}\")\n",
    "if 'Product' in policy_details:\n",
    "    print(f\"  Product: {policy_details['Product']}\")\n",
    "\n",
    "# Waterfall plot for this prediction\n",
    "plt.figure(figsize=(12, 8))\n",
    "shap.waterfall_plot(\n",
    "    shap.Explanation(\n",
    "        values=shap_values_single[predicted_cluster][0],\n",
    "        base_values=explainer.expected_value[predicted_cluster],\n",
    "        data=single_policy[0],\n",
    "        feature_names=preprocessor.feature_names\n",
    "    ),\n",
    "    max_display=15,\n",
    "    show=False\n",
    ")\n",
    "plt.title(f'SHAP Explanation for Policy {policy_idx} - Cluster {predicted_cluster}', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Production-Ready Similarity Search Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_policies_for_underwriter(new_policy_data, top_n=3, explain=True):\n",
    "    \"\"\"\n",
    "    Production function to find similar policies for underwriter review\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    new_policy_data : dict or DataFrame\n",
    "        New policy data with same features as training data\n",
    "    top_n : int\n",
    "        Number of similar policies to return (default: 3)\n",
    "    explain : bool\n",
    "        Whether to include SHAP explanations (default: True)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Contains similar policies and explanations\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert to DataFrame if dict\n",
    "    if isinstance(new_policy_data, dict):\n",
    "        new_policy_df = pd.DataFrame([new_policy_data])\n",
    "    else:\n",
    "        new_policy_df = new_policy_data.copy()\n",
    "    \n",
    "    # Preprocess the new policy\n",
    "    # Note: In production, you'd use the fitted preprocessor\n",
    "    X_new, _ = preprocessor.fit_transform(\n",
    "        new_policy_df,\n",
    "        numerical_features,\n",
    "        available_categorical\n",
    "    )\n",
    "    \n",
    "    # Apply dimensionality reduction\n",
    "    X_new_pca = pca.transform(X_new)\n",
    "    X_new_umap = umap_reducer_cluster.transform(X_new_pca)\n",
    "    \n",
    "    # Find similar policies\n",
    "    similar_policies = matcher.find_similar_policies(X_new_umap[0])\n",
    "    top_similar = similar_policies.iloc[:top_n]\n",
    "    \n",
    "    # Prepare result\n",
    "    result = {\n",
    "        'query_policy': new_policy_df.iloc[0].to_dict(),\n",
    "        'similar_policies': [],\n",
    "        'cluster_prediction': rf_explainer.predict(X_new)[0]\n",
    "    }\n",
    "    \n",
    "    # Format similar policies\n",
    "    for idx, (_, policy) in enumerate(top_similar.iterrows(), 1):\n",
    "        policy_info = {\n",
    "            'rank': idx,\n",
    "            'similarity_score': policy['similarity_score'],\n",
    "            'policy_tiv': policy['policy_tiv'],\n",
    "            'revenue': policy['Revenue'],\n",
    "            'employees': policy['EMP_TOT'],\n",
    "            'cluster': policy['cluster']\n",
    "        }\n",
    "        \n",
    "        if 'Product' in policy:\n",
    "            policy_info['product'] = policy['Product']\n",
    "        if 'Policy Industry Description' in policy:\n",
    "            policy_info['industry'] = policy['Policy Industry Description']\n",
    "            \n",
    "        result['similar_policies'].append(policy_info)\n",
    "    \n",
    "    # Add SHAP explanation if requested\n",
    "    if explain:\n",
    "        shap_vals = explainer.shap_values(X_new)\n",
    "        predicted_cluster = result['cluster_prediction']\n",
    "        \n",
    "        # Get top contributing features\n",
    "        feature_contributions = pd.DataFrame({\n",
    "            'feature': preprocessor.feature_names,\n",
    "            'shap_value': shap_vals[predicted_cluster][0]\n",
    "        }).sort_values('shap_value', key=abs, ascending=False).head(10)\n",
    "        \n",
    "        result['explanation'] = feature_contributions.to_dict('records')\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Example usage\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"PRODUCTION FUNCTION DEMONSTRATION\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Create a sample new policy\n",
    "sample_new_policy = df.sample(1).to_dict('records')[0]\n",
    "\n",
    "result = find_similar_policies_for_underwriter(\n",
    "    sample_new_policy, \n",
    "    top_n=3, \n",
    "    explain=True\n",
    ")\n",
    "\n",
    "print(f\"\\nQuery Policy TIV: ${result['query_policy']['policy_tiv']:,.0f}\")\n",
    "print(f\"Predicted Cluster: {result['cluster_prediction']}\")\n",
    "\n",
    "print(f\"\\n{'='*100}\")\n",
    "print(\"TOP 3 SIMILAR POLICIES:\")\n",
    "print(f\"{'='*100}\")\n",
    "\n",
    "for policy in result['similar_policies']:\n",
    "    print(f\"\\nRank {policy['rank']} - Similarity Score: {policy['similarity_score']:.3f}\")\n",
    "    print(f\"  TIV: ${policy['policy_tiv']:,.0f}\")\n",
    "    print(f\"  Revenue: ${policy['revenue']:,.0f}\")\n",
    "    print(f\"  Employees: {policy['employees']:.0f}\")\n",
    "    print(f\"  Cluster: {policy['cluster']}\")\n",
    "\n",
    "print(f\"\\n{'='*100}\")\n",
    "print(\"TOP 10 CONTRIBUTING FEATURES (SHAP):\")\n",
    "print(f\"{'='*100}\")\n",
    "\n",
    "for contrib in result['explanation']:\n",
    "    print(f\"  {contrib['feature']:<30} SHAP Value: {contrib['shap_value']:>8.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary & Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘                 INSURANCE POLICY SIMILARITY SYSTEM                           â•‘\n",
    "â•‘                        SUMMARY & RECOMMENDATIONS                             â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "ğŸ“Š MODEL OVERVIEW:\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "\n",
    "1. PREPROCESSING:\n",
    "   â€¢ Engineered features: Financial ratios, company age, location consistency\n",
    "   â€¢ Categorical encoding: Frequency + Label encoding\n",
    "   â€¢ Scaling: RobustScaler (handles outliers)\n",
    "   â€¢ Total features processed: {}\n",
    "\n",
    "2. DIMENSIONALITY REDUCTION:\n",
    "   â€¢ PCA: Reduced to {} components ({:.1f}% variance retained)\n",
    "   â€¢ UMAP: 10D for clustering, 2D for visualization\n",
    "   \n",
    "3. CLUSTERING:\n",
    "   â€¢ Algorithm: K-Means (best performance)\n",
    "   â€¢ Optimal clusters: {}\n",
    "   â€¢ Silhouette score: {:.3f}\n",
    "   â€¢ Davies-Bouldin score: {:.3f}\n",
    "\n",
    "4. SIMILARITY SEARCH:\n",
    "   â€¢ Method: K-Nearest Neighbors\n",
    "   â€¢ Metric: Euclidean distance\n",
    "   â€¢ Cluster consistency: {:.1f}%\n",
    "   â€¢ Product consistency: {:.1f}%\n",
    "\n",
    "5. EXPLAINABILITY:\n",
    "   â€¢ SHAP values computed for feature importance\n",
    "   â€¢ Random Forest surrogate model accuracy: {:.1f}%\n",
    "   â€¢ Top features identified and ranked\n",
    "\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "\n",
    "ğŸ’¡ KEY RECOMMENDATIONS:\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "\n",
    "FOR PRODUCTION DEPLOYMENT:\n",
    "\n",
    "1. DATA QUALITY:\n",
    "   âœ“ Ensure consistent data collection for all features\n",
    "   âœ“ Implement data validation pipelines\n",
    "   âœ“ Handle missing values systematically\n",
    "   âœ“ Monitor for data drift over time\n",
    "\n",
    "2. MODEL MAINTENANCE:\n",
    "   âœ“ Retrain model quarterly or when significant market changes occur\n",
    "   âœ“ Track model performance metrics over time\n",
    "   âœ“ A/B test with underwriters to validate recommendations\n",
    "   âœ“ Collect feedback loop from underwriters\n",
    "\n",
    "3. FEATURE ENGINEERING:\n",
    "   âœ“ Consider adding claim history features if available\n",
    "   âœ“ Include temporal features (policy duration, renewal count)\n",
    "   âœ“ Add external data: economic indicators, industry trends\n",
    "   âœ“ Create interaction features between key variables\n",
    "\n",
    "4. SCALING:\n",
    "   âœ“ Use batch processing for large datasets\n",
    "   âœ“ Implement caching for frequently accessed policies\n",
    "   âœ“ Consider approximate nearest neighbors (Annoy, FAISS) for speed\n",
    "   âœ“ Deploy as REST API for easy integration\n",
    "\n",
    "5. EXPLAINABILITY:\n",
    "   âœ“ Provide SHAP explanations to underwriters\n",
    "   âœ“ Create visual dashboards for similarity insights\n",
    "   âœ“ Document feature importance for compliance\n",
    "   âœ“ Enable feature contribution drill-down\n",
    "\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "\n",
    "ğŸ¯ BUSINESS VALUE:\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "\n",
    "â€¢ FASTER UNDERWRITING: Reduce research time by providing relevant comparables\n",
    "â€¢ BETTER PRICING: Learn from similar historical policies\n",
    "â€¢ RISK ASSESSMENT: Identify patterns from past renewals\n",
    "â€¢ CONSISTENCY: Standardize evaluation across underwriters\n",
    "â€¢ KNOWLEDGE TRANSFER: Junior underwriters learn from historical data\n",
    "\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "\n",
    "ğŸ“ NEXT STEPS:\n",
    "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n",
    "\n",
    "1. Validate with actual data\n",
    "2. Pilot with select underwriters\n",
    "3. Gather feedback and iterate\n",
    "4. Deploy to production\n",
    "5. Monitor and improve continuously\n",
    "\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\"\"\".format(\n",
    "    len(preprocessor.feature_names),\n",
    "    n_components_pca,\n",
    "    pca.explained_variance_ratio_.sum() * 100,\n",
    "    optimal_k,\n",
    "    max(silhouette_scores),\n",
    "    min(davies_bouldin_scores),\n",
    "    eval_results['Cluster Consistency'] * 100,\n",
    "    eval_results['Product Consistency'] * 100,\n",
    "    rf_explainer.score(X, primary_labels) * 100\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Model Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import joblib\n",
    "\n",
    "# Save all model components\n",
    "model_artifacts = {\n",
    "    'preprocessor': preprocessor,\n",
    "    'pca': pca,\n",
    "    'umap_cluster': umap_reducer_cluster,\n",
    "    'umap_viz': umap_reducer,\n",
    "    'kmeans': kmeans,\n",
    "    'nn_model': matcher.nn_model,\n",
    "    'rf_explainer': rf_explainer,\n",
    "    'shap_explainer': explainer,\n",
    "    'feature_names': preprocessor.feature_names,\n",
    "    'cluster_profiles': cluster_profiles\n",
    "}\n",
    "\n",
    "# Save using joblib (better for sklearn objects)\n",
    "joblib.dump(model_artifacts, 'insurance_similarity_model.pkl')\n",
    "\n",
    "print(\"âœ“ Model artifacts saved successfully to 'insurance_similarity_model.pkl'\")\n",
    "print(\"\\nTo load in production:\")\n",
    "print(\"  model = joblib.load('insurance_similarity_model.pkl')\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
